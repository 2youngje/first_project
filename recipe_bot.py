import os
import uuid
from typing import List

import streamlit as st
from dotenv import load_dotenv

from langchain_core.documents import Document
from langchain_upstage import UpstageEmbeddings, ChatUpstage
from langchain_chroma import Chroma

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
api_key = os.getenv("UPSTAGE_API_KEY")

# ì„¸ì…˜ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
if "id" not in st.session_state:
    st.session_state.id = uuid.uuid4()

# ì•± ì œëª©
st.title("ğŸ³ìš”ë¦¬ ë ˆì‹œí”¼ ì±—ë´‡")
st.markdown("ì˜ˆ: â€˜ê¹€ì¹˜ë³¶ìŒë°¥ ë§Œë“œëŠ” ë²• ì•Œë ¤ì¤˜â€™, â€˜ë‘ë¶€ë¡œ ë§Œë“¤ ìˆ˜ ìˆëŠ” ìš”ë¦¬ ìˆì–´?â€™")

# ë ˆì‹œí”¼ ë°ì´í„° ì •ì˜
recipes: List[Document] = [
    Document(page_content="ê¹€ì¹˜ë³¶ìŒë°¥: ë°¥, ê¹€ì¹˜, ì°¸ê¸°ë¦„, ê°„ì¥, ì„¤íƒ•, ëŒ€íŒŒ, ê³„ë€ì„ ì‚¬ìš©í•´ ë³¶ìŒë°¥ì„ ë§Œë“ ë‹¤."),
    Document(page_content="ëœì¥ì°Œê°œ: ëœì¥, ë‘ë¶€, ì• í˜¸ë°•, ì–‘íŒŒ, ê³ ì¶”, ë§ˆëŠ˜ ë“±ì„ ë„£ê³  ë“ì¸ë‹¤."),
    Document(page_content="ê³„ë€ë§ì´: ê³„ë€, ë‹¹ê·¼, íŒŒ, ì†Œê¸ˆì„ ë„£ê³  ì–‡ê²Œ ë¶€ì³ ëŒëŒ ë§Œë‹¤."),
    Document(page_content="ë¶€ì¹¨ê°œ: ë¶€ì¹¨ê°€ë£¨, ë¬¼, ì•¼ì±„(ë¶€ì¶”, ì–‘íŒŒ ë“±), ì†Œê¸ˆì„ ë„£ê³  íŒ¬ì— ë¶€ì³ ì™„ì„±."),
    Document(page_content="ë–¡ë³¶ì´: ë–¡, ê³ ì¶”ì¥, ê³ ì¶§ê°€ë£¨, ì–´ë¬µ, ì–‘íŒŒ, ì„¤íƒ•ì„ ë„£ê³  ìì‘í•˜ê²Œ ë“ì¸ë‹¤."),
]

# ë²¡í„°ìŠ¤í† ì–´ ë° ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
vectorstore = Chroma.from_documents(recipes, UpstageEmbeddings(model="solar-embedding-1-large"))
retriever = vectorstore.as_retriever(k=2)
chat = ChatUpstage(upstage_api_key=api_key)

# í”„ë¡¬í”„íŠ¸ ì •ì˜
contextualize_q_system_prompt = """ì‚¬ìš©ìì˜ ìš”ë¦¬ ê´€ë ¨ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™”ì™€ ê´€ë ¨ì´ ìˆìœ¼ë©´, ë…ë¦½ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ êµ¬ì„±í•˜ì„¸ìš”. ë‹µë³€ì€ í•˜ì§€ ë§ˆì„¸ìš”."""
contextualize_q_prompt = ChatPromptTemplate.from_messages([
    ("system", contextualize_q_system_prompt),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}")
])
history_aware_retriever = create_history_aware_retriever(chat, retriever, contextualize_q_prompt)

qa_system_prompt = """ìš”ë¦¬ ì „ë¬¸ê°€ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ ì œê³µëœ ë ˆì‹œí”¼ ë‚´ìš©ì„ í™œìš©í•˜ì„¸ìš”. ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ê³ , ë‹µë³€ì€ ì„¸ ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
ğŸ“ë‹µë³€ ë‚´ìš©:
ğŸ“ì¦ê±°:
{context}
"""
qa_prompt = ChatPromptTemplate.from_messages([
    ("system", qa_system_prompt),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}")
])
question_answer_chain = create_stuff_documents_chain(chat, qa_prompt)
rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

# ê¸°ì¡´ ë©”ì„¸ì§€ ì¶œë ¥
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì±„íŒ… ì…ë ¥
if prompt := st.chat_input("ìš”ë¦¬ì— ëŒ€í•´ ê¶ê¸ˆí•œ ê±¸ ë¬¼ì–´ë³´ì„¸ìš”!"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""

        result = rag_chain.invoke({
            "input": prompt,
            "chat_history": st.session_state.messages,
        })

        with st.expander("ğŸ” ì°¸ê³ í•œ ë ˆì‹œí”¼ ë¬¸ì„œ ë³´ê¸°"):
            st.write(result["context"])

        for chunk in result["answer"].split(" "):
            full_response += chunk + " "
            message_placeholder.markdown(full_response + "â–Œ")
        message_placeholder.markdown(full_response)

    st.session_state.messages.append({"role": "assistant", "content": full_response})

# ì´ˆê¸° : ì‚¬ì´ë“œ ë°” ì œê±°(ë©”ì¸ í™”ë©´ì—ì„œ êµ¬ë™ í•  ìˆ˜ ìˆê²Œ)